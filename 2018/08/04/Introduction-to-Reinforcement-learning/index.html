<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="default">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="强化学习什么是强化学习介于监督学习（有label）和无监督学习的中间

没有监督信息，只有“奖赏”的反馈信号
反馈通常是延迟的
样本通常不符合独立同分布假设
智能体的动作会影响到其后续观察到的数据的分布

Example：

直升机的飞行控制
+r 沿着既定路线飞行
-r 偏离或坠毁


组合投资管理
+r 盈利
-r 亏损


电站的控制
+r 稳定输出电力
-r 超出安全预警


Atari游">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Reinforcement learning">
<meta property="og:url" content="http://yoursite.com/2018/08/04/Introduction-to-Reinforcement-learning/index.html">
<meta property="og:site_name" content="Ace">
<meta property="og:description" content="强化学习什么是强化学习介于监督学习（有label）和无监督学习的中间

没有监督信息，只有“奖赏”的反馈信号
反馈通常是延迟的
样本通常不符合独立同分布假设
智能体的动作会影响到其后续观察到的数据的分布

Example：

直升机的飞行控制
+r 沿着既定路线飞行
-r 偏离或坠毁


组合投资管理
+r 盈利
-r 亏损


电站的控制
+r 稳定输出电力
-r 超出安全预警


Atari游">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/04de9a06-2330-4433-a858-3d72dcbeae9b/1522652096508.jpg">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/79655275-f0e8-410c-b5ed-b6a634266d71/1522652097371.jpg">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/a7ac93ad-71ae-4e93-b0c7-a0e17966c645/1522652097931.jpg">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/341b7df1-2943-4838-a1d6-39c3b971b709/1522652100958.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-8381e1ee9bdec6720629ce5e06c29baa_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-77f230260004d6935a8c7a0816d29db5_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-cadfd4a98f96e815214a0a431ca343a8_hd.jpg">
<meta property="og:updated_time" content="2018-08-05T02:40:50.277Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Introduction to Reinforcement learning">
<meta name="twitter:description" content="强化学习什么是强化学习介于监督学习（有label）和无监督学习的中间

没有监督信息，只有“奖赏”的反馈信号
反馈通常是延迟的
样本通常不符合独立同分布假设
智能体的动作会影响到其后续观察到的数据的分布

Example：

直升机的飞行控制
+r 沿着既定路线飞行
-r 偏离或坠毁


组合投资管理
+r 盈利
-r 亏损


电站的控制
+r 稳定输出电力
-r 超出安全预警


Atari游">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/editor/04de9a06-2330-4433-a858-3d72dcbeae9b/1522652096508.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/04/Introduction-to-Reinforcement-learning/"/>





  <title>Introduction to Reinforcement learning | Ace</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ace</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Ace.github.io</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/04/Introduction-to-Reinforcement-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ace_ZAJ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/me.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ace">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Introduction to Reinforcement learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-04T18:29:04+08:00">
                2018-08-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><h3 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h3><p>介于监督学习（有label）和无监督学习的中间</p>
<ul>
<li>没有监督信息，只有“奖赏”的反馈信号</li>
<li>反馈通常是延迟的</li>
<li>样本通常不符合独立同分布假设</li>
<li>智能体的动作会影响到其后续观察到的数据的分布</li>
</ul>
<p>Example：</p>
<ul>
<li>直升机的飞行控制<ul>
<li>+r 沿着既定路线飞行</li>
<li>-r 偏离或坠毁</li>
</ul>
</li>
<li>组合投资管理<ul>
<li>+r 盈利</li>
<li>-r 亏损</li>
</ul>
</li>
<li>电站的控制<ul>
<li>+r 稳定输出电力</li>
<li>-r 超出安全预警</li>
</ul>
</li>
<li>Atari游戏的控制<ul>
<li>+r 增加游戏分数</li>
<li>-r 减小游戏分数</li>
</ul>
</li>
</ul>
<p><strong>序列决策问题</strong></p>
<h4 id="奖赏-Reward"><a href="#奖赏-Reward" class="headerlink" title="奖赏 Reward"></a>奖赏 Reward</h4><p>用于评价智能体在给定任务上动作的“奖励”：</p>
<ul>
<li>通常是一个标量</li>
<li>在智能体决策的每一步，都会得到环境对其的奖赏</li>
<li>智能体的目标是最大化累积奖赏</li>
</ul>
<p>问题可以用强化学习建模：</p>
<p>​    任务的目标可以等价的表示为在某个奖赏函数上的累积最大化</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">          ----观察o1----&gt;</div><div class="line"></div><div class="line">          &lt;----动作a1----  </div><div class="line"></div><div class="line">          ----奖赏r1,o2----&gt;</div><div class="line"></div><div class="line">环境      &lt;----动作a2----    智能体</div><div class="line"></div><div class="line">          ----奖赏r2,o3----&gt;</div><div class="line"></div><div class="line">          &lt;----动作a3----</div><div class="line"></div><div class="line">          ----奖赏r3,o4----&gt;</div></pre></td></tr></table></figure>
<p>$\max\sum_{i=1}^{T}r_i$</p>
<p><strong>目标：</strong></p>
<ul>
<li>通过选择动作最大化累计奖赏</li>
<li>奖赏可能得到延迟</li>
<li>有时需要牺牲眼前的小利益获得长远的大利益</li>
</ul>
<h4 id="状态-state"><a href="#状态-state" class="headerlink" title="状态 state"></a>状态 state</h4><p>历史定义为，t时刻之前能观察到的所有信息：</p>
<p>$$ a_1, r_1,…o_t$$</p>
<p>状态是决定接下来会发生什么的关键信息</p>
<ul>
<li>环境状态 $S_t^e$<ul>
<li>环境的私有表示信息</li>
<li>决定了下一步环境反馈什么观察和奖赏给智能体</li>
<li>通常对智能体不是完全可见的</li>
<li>即使完全可见，对智能体来说含有大量的冗余信息</li>
</ul>
</li>
<li>智能体状态$S_t^a$<ul>
<li>智能体的内部表示</li>
<li>决定了下一步智能体选择什么动作</li>
<li>通常是强化学习算法的输入</li>
<li>其可以表示为历史的一个函数，即$S_t^a = f(H_t)$</li>
</ul>
</li>
</ul>
<h3 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h3><p>马尔科夫链：</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/04de9a06-2330-4433-a858-3d72dcbeae9b/1522652096508.jpg" alt="Markov"></p>
<p>一个状态是马尔可夫的，当且仅当</p>
<p>$$P(S_{t+1}|S_t) = P(S_{t+1}|S1, S2,…St)$$</p>
<p>给定现在，未来与历史无关</p>
<p>环境的状态$S_t^e, H_t$都是马尔可夫的</p>
<p>全部可观察性：</p>
<p>​    当$o_t=s_t^a=s_t^e$时，我们便得到了一个马尔科夫随机过程（MDP）</p>
<h4 id="马尔科夫过程-MP"><a href="#马尔科夫过程-MP" class="headerlink" title="马尔科夫过程 MP"></a>马尔科夫过程 MP</h4><p>Definition: A Markov Process(or Markov Chain) is a tuple(S, P) 状态空间和概率转移矩阵</p>
<ul>
<li><p>S is a (finite) set of states</p>
</li>
<li><p>P is a state transition probability matrix</p>
<p>$P_{ss’}=P{S_{t+1=s’}|S_t=s}$</p>
</li>
</ul>
<h4 id="马尔科夫奖赏过程-MRP"><a href="#马尔科夫奖赏过程-MRP" class="headerlink" title="马尔科夫奖赏过程 MRP"></a>马尔科夫奖赏过程 MRP</h4><p>A Markov Reward Process is a tuple $(S, P, R, r)$</p>
<ul>
<li><p>S is a finite set of states</p>
</li>
<li><p>P is a state transition probability matrix</p>
<p>$P_{ss’}=P{S_{t+1=s’}|S_t=s}$</p>
</li>
<li><p>R is a reward function</p>
<p>$R_s = E{R_{t+1}|S_t=s}$</p>
</li>
<li><p>r is a discount factor, $r \in [0,1]$</p>
</li>
</ul>
<h4 id="马尔科夫决策过程-MDP"><a href="#马尔科夫决策过程-MDP" class="headerlink" title="马尔科夫决策过程 MDP"></a>马尔科夫决策过程 MDP</h4><p><img src="https://image.jiqizhixin.com/uploads/editor/79655275-f0e8-410c-b5ed-b6a634266d71/1522652097371.jpg" alt=""></p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/a7ac93ad-71ae-4e93-b0c7-a0e17966c645/1522652097931.jpg" alt=""></p>
<p>A Markov Decision Process is a tuple $(S, A,P, R, r)$</p>
<ul>
<li><p>S is a finite set of states</p>
</li>
<li><p>A is a finite set of actions</p>
</li>
<li><p>P is a state transition probability matrix</p>
<p>$P_{ss’}^a=P{S_{t+1=s’}|S_t=s, A_t=a}$</p>
</li>
<li><p>R is a reward function</p>
<p>$R_s^a = E{R_{t+1}|S_t=s, A_t=a}$</p>
</li>
<li><p>r is a discount factor, $r \in [0,1]$</p>
</li>
</ul>
<p>Example</p>
<h4 id="马尔科夫随机过程-MDP"><a href="#马尔科夫随机过程-MDP" class="headerlink" title="马尔科夫随机过程 MDP"></a>马尔科夫随机过程 MDP</h4><p>$V(sunny) = E[\sum_{t=1}^Tr_t|S_0=sunny]$</p>
<p>$V(sunny) = E[\sum_{t=1}^\infty \gamma^tr_t|S_0=sunny]$</p>
<p>状态值函数</p>
<h4 id="马尔科夫决策过程-1"><a href="#马尔科夫决策过程-1" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h4><p><strong>策略(policy)</strong>的引入：状态到动作的映射</p>
<p>随机策略 $\pi(a|s)=P(a|s)$</p>
<p>确定性策略 $\pi(s)=argmax_aP(a|s)$</p>
<h4 id="MDP的策略评估"><a href="#MDP的策略评估" class="headerlink" title="MDP的策略评估"></a>MDP的策略评估</h4><h5 id="状态值函数-V"><a href="#状态值函数-V" class="headerlink" title="状态值函数 V"></a>状态值函数 V</h5><p>从状态s开始，使用当前策略进行后续动作选择，得到的期望奖赏</p>
<p>$V^\pi(s)=E_\pi[\sum_{k=0}^\infty \gamma_k R_{t+k+1}|S_t=s]$</p>
<p>$V^\pi(s)=E_\pi[R_{t+1}+\gamma V^\pi(S_{t+1})|S_t=s]$  (递归表示)</p>
<h5 id="状态-动作值函数-Q"><a href="#状态-动作值函数-Q" class="headerlink" title="状态-动作值函数 Q"></a>状态-动作值函数 Q</h5><p>从状态s开始，执行动作a，并使用当前策略进行后续选择</p>
<p>$Q^\pi(s,a)=E_\pi[{\sum_{k=0}^\infty \gamma^k R_{t+k+1}|s_t=s,a_t=a}]$</p>
<h5 id="值函数转换关系-Bellman-Expectation-Equation"><a href="#值函数转换关系-Bellman-Expectation-Equation" class="headerlink" title="值函数转换关系 Bellman Expectation Equation"></a>值函数转换关系 Bellman Expectation Equation</h5><p>$V^\pi(s)=\sum_a\pi(a|s)Q^\pi(s,a)$</p>
<p>$Q^\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^\pi(s^{\prime})$</p>
<p>=&gt;</p>
<p>$V^\pi(s)=\sum_a\pi(a|s)(R(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^\pi(s^{\prime}))$</p>
<p>$Q^\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)\sum_a\pi(a|s)Q^\pi(s^\prime,a^\prime)$</p>
<h5 id="最优状态值函数"><a href="#最优状态值函数" class="headerlink" title="最优状态值函数"></a>最优状态值函数</h5><p>$$V^*(s)=\max_\pi V^\pi(s)$$</p>
<h5 id="最优状态-动作函数"><a href="#最优状态-动作函数" class="headerlink" title="最优状态-动作函数"></a>最优状态-动作函数</h5><p>$$Q^*(s,a)=\max_\pi Q^\pi(s,a)$$$</p>
<ul>
<li>最优函数值决定了MDP中可以达到的最优性能</li>
<li>当最优函数值已知时，该MDP已解</li>
</ul>
<h5 id="定义策略上的偏序关系"><a href="#定义策略上的偏序关系" class="headerlink" title="定义策略上的偏序关系"></a>定义策略上的偏序关系</h5><p>定理：对任意的MDP有</p>
<ul>
<li>至少存在一个最优策略$\pi^<em>$优于或不差于所有其他策略，即$\pi^</em> \ge \pi, \forall \pi$</li>
<li>所有的<strong>最优策略</strong>都可以取到最优状态值函数 $V^{\pi^<em>}=V^</em>(s)$</li>
<li>所有的最优策略都可以取到最优状态-动作值函数 $Q^{\pi^<em>}(s,a) = Q^</em>(s,a)$</li>
</ul>
<h5 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h5><p>最优策略可以通过最大化Q函数得到，即</p>
<p>$\pi^<em>(s)=argmax_aQ^{\pi^</em>}(s,a)$</p>
<ul>
<li>对于任意MDP，总是存在一个确定性的最优策略</li>
<li>如果最优Q函数已知，则最优策略已知</li>
</ul>
<h5 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h5><p><strong>Bellman</strong>最优方程</p>
<p>$$V^<em>(s)=max_aQ^</em>(s,a)$$</p>
<p>$$Q^<em>(s,a)=R(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^</em>(s^{\prime})$$</p>
<p>=&gt;</p>
<p>$V^\pi(s)=max_aR(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^*(s^{\prime})$</p>
<p>$Q^<em>(s,a)=R(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)\max_a\pi(a|s)Q^</em>(s^\prime,a^\prime)$</p>
<ul>
<li>没有闭式解，没有一个式子可以直接解出来</li>
<li>可以迭代求解：值迭代，策略迭代，Q-learning，SARSA</li>
</ul>
<h3 id="强化学习算法"><a href="#强化学习算法" class="headerlink" title="强化学习算法"></a>强化学习算法</h3><h5 id="基于模型的方法"><a href="#基于模型的方法" class="headerlink" title="基于模型的方法"></a>基于模型的方法</h5><p>方法概述：</p>
<p>​    转移概率和奖赏函数未知，用函数逼近的方法拟合转移概率和奖赏函数，变成标准的MDP问题，然后使用DP planning方法求解</p>
<p>特点：</p>
<ul>
<li>模型拟合通常是收敛的</li>
<li>并不保证拟合模型上的最优策略是真实的最优</li>
</ul>
<p>代表算法：</p>
<ul>
<li>Dyna, GPS</li>
</ul>
<p>研究较少，因为大部分问题难以建模，转移概率和奖赏函数非常复杂</p>
<h5 id="基于值函数的方法"><a href="#基于值函数的方法" class="headerlink" title="基于值函数的方法"></a>基于值函数的方法</h5><p>方法概述：</p>
<p>​    通过估计值函数来隐式得到策略</p>
<p>特点：</p>
<ul>
<li>最好情况下可以最小化Bellman误差</li>
<li>非线性情况下通常不保证收敛</li>
</ul>
<p>代表算法：</p>
<ul>
<li>Q-Learning, TD</li>
</ul>
<h5 id="基于策略梯度的方法"><a href="#基于策略梯度的方法" class="headerlink" title="基于策略梯度的方法"></a>基于策略梯度的方法</h5><p>方法概述：</p>
<p>​    直接对目标进行梯度推导，通常是sample，执行梯度方法进行策略提升</p>
<p>特点：</p>
<ul>
<li>避免值函数估计带来的<strong>策略退化</strong>（并不是loss回归误差越小，策略越好）</li>
<li>直接对目标进行优化</li>
</ul>
<p>代表算法：</p>
<ul>
<li>Reinforce, TRPO</li>
</ul>
<h5 id="Actor-Critic类方法"><a href="#Actor-Critic类方法" class="headerlink" title="Actor-Critic类方法"></a>Actor-Critic类方法</h5><p>结合值函数，策略梯度的方法</p>
<p>方法概述：</p>
<p>​    估计当前策略的值函数，并通过其来提升策略本身，通常是和策略梯度方法结合在一起</p>
<p>特点：</p>
<p>​    同时结合了值函数和策略梯度</p>
<p>代表算法：</p>
<p>​    A3C</p>
<h5 id="样本复杂度"><a href="#样本复杂度" class="headerlink" title="样本复杂度"></a>样本复杂度</h5><p>model-based shallow RL -&gt; model-based deep RL -&gt; off policy Q-function learning -&gt; actor-critic style methods -&gt; on policy gradient algorithms -&gt; evolutionary or gradient-free algorithms</p>
<p>more efficient (fewer samples) -&gt; less efficient (more samples)</p>
<h3 id="基于值函数估计的强化学习算法"><a href="#基于值函数估计的强化学习算法" class="headerlink" title="基于值函数估计的强化学习算法"></a>基于值函数估计的强化学习算法</h3><h5 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h5><p><img src="https://image.jiqizhixin.com/uploads/editor/341b7df1-2943-4838-a1d6-39c3b971b709/1522652100958.jpg" alt=""></p>
<p>初始的Q值 -&gt; 得到一个策略 -&gt; 从环境中采样 -&gt; Q值 -&gt;…</p>
<p>fit $V(s)$ or $Q(s,a)$ -&gt; set $\pi^<em>(s)=argmax_aQ^{\pi^</em>}(s,a)$</p>
<p>核心在估计值函数的计算/估计，第二步set较为简单</p>
<h5 id="蒙特卡洛采样"><a href="#蒙特卡洛采样" class="headerlink" title="蒙特卡洛采样"></a>蒙特卡洛采样</h5><p><strong>Monte-Carlo Method:</strong></p>
<p>$$Q^\pi(s,a)=\frac{1}{m}\sum_{i=1}^mR(\tau_i)$$</p>
<p>$\tau_i$ is sample by following $\tau$ after $s,a$, 采样的轨迹</p>
<p><strong>多次采样，效率低，无法泛化</strong></p>
<p><strong>Monte-Carlo update:</strong></p>
<p>$$Q(s_t,a_t)=Q(s_t,a_t)+\alpha(R-Q(s_t,a_t))$$</p>
<p>MC error: $\alpha(R-Q(s_t,a_t))$</p>
<p>增量更新，R从t+1开始，是轨迹上的，需要环境中run很多步，得到一个轨迹，到达终止状态或人为设置的步数，所以代价也很大</p>
<p><strong>Temporal difference Method:</strong> TDerror</p>
<p>$$Q(s_t,a_t)=Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$$</p>
<ul>
<li><p>TD error: $r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)$</p>
</li>
<li><p>$R$ – $r_{t+1}+\gamma Q(s_{t+1},a_{t+1})$</p>
<p>t+1之后的就不参加，用已有的Q估计代替，采样就只有一条</p>
</li>
</ul>
<p><strong>采样代价小，可泛化，参数化</strong></p>
<h5 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h5><p>$Q^\pi(s,a)$ based on $\pi$</p>
<p>$Q(s,a)$ $-\max_aQ(s,a)$</p>
<p><strong>off-policy TD control</strong></p>
<p>$Q_o=0$, initial state<br>for $i=0,1,..$</p>
<p>​    $s’,r$ = do action from policy $\pi_\epsilon$</p>
<p>​    $a’=\pi(s’)$</p>
<p>​    $Q^\pi(s,a)+=\alpha(r+\gamma Q^\pi(s’,a’)-Q(s,a))$</p>
<p>​    $\pi(s)=argmax_a Q(s,a)$</p>
<p>​    $s=s’, a=a’$</p>
<p>end for</p>
<p>目前在模型中学习到的policy</p>
<h5 id="on-policy-TD-control"><a href="#on-policy-TD-control" class="headerlink" title="on-policy TD control"></a>on-policy TD control</h5><p>$Q_0=0$, initial state</p>
<p>for $i=0,1,…$</p>
<p>​    $s’, r$=do action from policy $\pi_\epsilon$</p>
<p>​    $a’=\pi_\epsilon(s’)$</p>
<p>​    $Q^\pi(s,a)+=\alpha(r+\gamma Q^\pi(s’,a’)-Q(s,a))$</p>
<p>​    $\pi(s)=argmax_a Q(s,a)$</p>
<p>​    $s=s’, a=a’$</p>
<p>end for    </p>
<p>a是执行采样的policy得到的,$\pi_\epsilon$参数化的policy</p>
<ul>
<li>off policy：able to improve the policy without generating new samples from that policy</li>
<li>on policy: each time the policy is changed, even a little bit, we need to generate new samples</li>
</ul>
<p>如果空间连续，不能这样做</p>
<h5 id="值函数估计"><a href="#值函数估计" class="headerlink" title="值函数估计"></a>值函数估计</h5><p>可以将Q用参数表示，w</p>
<p>Q-learning with function approximation</p>
<p>$w=0$, initial state</p>
<p>for $i=0,1,…$</p>
<p>​    $s’,r=$ do action from policy $\pi_\epsilon$</p>
<p>​    $a’=\pi(s’)$</p>
<p>​    $w+=$ $\theta(r+\gamma\hat Q(s,a)-\hat Q(s,a)) \nabla(s_t, a_t)$</p>
<p>​    $\pi(s)=argmax_a Q(s,a)$</p>
<p>​    $s=s’, a=a’$</p>
<p>end for</p>
<p>$J(w)=E_{s\sim\pi}[(Q^\pi(s,a)-\hat Q(s,a))^2]$</p>
<p>$\hat Q(s,a)$– when using DNN model, it will be Deep Q-learning(DQN)</p>
<h3 id="基于策略梯度的强化学习算法"><a href="#基于策略梯度的强化学习算法" class="headerlink" title="基于策略梯度的强化学习算法"></a>基于策略梯度的强化学习算法</h3><p><img src="https://pic4.zhimg.com/80/v2-8381e1ee9bdec6720629ce5e06c29baa_hd.jpg" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-77f230260004d6935a8c7a0816d29db5_hd.jpg" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-cadfd4a98f96e815214a0a431ca343a8_hd.jpg" alt=""></p>
<p>核心在梯度的计算</p>
<p>目标函数-&gt;梯度-&gt;优化</p>
<p>$$J(\theta)=\int_\tau {p_\theta(\tau)R(\tau)} \,{\rm d}\tau$$</p>
<p>$\theta$ , policy的参数，$J$ 是策略在所有轨迹$\tau$的概率的期望</p>
<p>$$P_\theta(\tau)=p(s_0)\prod_{i=0}^{T-1}p(s_{i+1}|a_i,s_i)\pi_\theta(a_i|s_i)$$</p>
<p>$\nabla_\theta J=\int_\tau {p_\theta(\tau)\nabla_\theta p_\theta(\tau) \log R(\tau)} \,{\rm d}\tau$</p>
<p>trick: $\nabla_\theta p_\theta(\tau) =p_\theta(\tau)\nabla_\theta\log p_\theta(\tau) $</p>
<p>$p\nabla_\theta\log p_\theta(\tau)=$ …</p>
<p>REINFORCE 算法</p>
<p>…</p>
<p>另一种表达（在稳定分布上积分）</p>
<p>…</p>
<p>A3C</p>
<p>DDPG</p>
<p>TRPO</p>
<p>PPO</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/04/Introduction-to-Reinforcement-learning2/" rel="next" title="Introduction to Reinforcement learning">
                <i class="fa fa-chevron-left"></i> Introduction to Reinforcement learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/04/U-Net在CT图像分割的应用/" rel="prev" title="U-Net在CT图像分割的应用">
                U-Net在CT图像分割的应用 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/me.JPG"
                alt="Ace_ZAJ" />
            
              <p class="site-author-name" itemprop="name">Ace_ZAJ</p>
              <p class="site-description motion-element" itemprop="description">boom pow</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">108</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#强化学习"><span class="nav-number">1.</span> <span class="nav-text">强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是强化学习"><span class="nav-number">1.1.</span> <span class="nav-text">什么是强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#奖赏-Reward"><span class="nav-number">1.1.1.</span> <span class="nav-text">奖赏 Reward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#状态-state"><span class="nav-number">1.1.2.</span> <span class="nav-text">状态 state</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#马尔科夫决策过程"><span class="nav-number">1.2.</span> <span class="nav-text">马尔科夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫过程-MP"><span class="nav-number">1.2.1.</span> <span class="nav-text">马尔科夫过程 MP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫奖赏过程-MRP"><span class="nav-number">1.2.2.</span> <span class="nav-text">马尔科夫奖赏过程 MRP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫决策过程-MDP"><span class="nav-number">1.2.3.</span> <span class="nav-text">马尔科夫决策过程 MDP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫随机过程-MDP"><span class="nav-number">1.2.4.</span> <span class="nav-text">马尔科夫随机过程 MDP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#马尔科夫决策过程-1"><span class="nav-number">1.2.5.</span> <span class="nav-text">马尔科夫决策过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MDP的策略评估"><span class="nav-number">1.2.6.</span> <span class="nav-text">MDP的策略评估</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#状态值函数-V"><span class="nav-number">1.2.6.1.</span> <span class="nav-text">状态值函数 V</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#状态-动作值函数-Q"><span class="nav-number">1.2.6.2.</span> <span class="nav-text">状态-动作值函数 Q</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#值函数转换关系-Bellman-Expectation-Equation"><span class="nav-number">1.2.6.3.</span> <span class="nav-text">值函数转换关系 Bellman Expectation Equation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#最优状态值函数"><span class="nav-number">1.2.6.4.</span> <span class="nav-text">最优状态值函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#最优状态-动作函数"><span class="nav-number">1.2.6.5.</span> <span class="nav-text">最优状态-动作函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#定义策略上的偏序关系"><span class="nav-number">1.2.6.6.</span> <span class="nav-text">定义策略上的偏序关系</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#最优策略"><span class="nav-number">1.2.6.7.</span> <span class="nav-text">最优策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#最优值函数"><span class="nav-number">1.2.6.8.</span> <span class="nav-text">最优值函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习算法"><span class="nav-number">1.3.</span> <span class="nav-text">强化学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基于模型的方法"><span class="nav-number">1.3.0.1.</span> <span class="nav-text">基于模型的方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于值函数的方法"><span class="nav-number">1.3.0.2.</span> <span class="nav-text">基于值函数的方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于策略梯度的方法"><span class="nav-number">1.3.0.3.</span> <span class="nav-text">基于策略梯度的方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Actor-Critic类方法"><span class="nav-number">1.3.0.4.</span> <span class="nav-text">Actor-Critic类方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#样本复杂度"><span class="nav-number">1.3.0.5.</span> <span class="nav-text">样本复杂度</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于值函数估计的强化学习算法"><span class="nav-number">1.4.</span> <span class="nav-text">基于值函数估计的强化学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#过程"><span class="nav-number">1.4.0.1.</span> <span class="nav-text">过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#蒙特卡洛采样"><span class="nav-number">1.4.0.2.</span> <span class="nav-text">蒙特卡洛采样</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Q-learning"><span class="nav-number">1.4.0.3.</span> <span class="nav-text">Q-learning</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#on-policy-TD-control"><span class="nav-number">1.4.0.4.</span> <span class="nav-text">on-policy TD control</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#值函数估计"><span class="nav-number">1.4.0.5.</span> <span class="nav-text">值函数估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于策略梯度的强化学习算法"><span class="nav-number">1.5.</span> <span class="nav-text">基于策略梯度的强化学习算法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ace_ZAJ</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="default">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="AutoML," />










<meta name="description" content="[TOC]
Automated Machine Learning
Key words: automatic machine learning, neural architecture search, hyper-parameter optimization, meta-learning, transfer-learning
1. IntroductionEvery aspect of machin">
<meta property="og:type" content="article">
<meta property="og:title" content="Taking Human out of Learning Applications: A Survey on Automated Machine Learning">
<meta property="og:url" content="http://yoursite.com/2018/11/11/Taking-Human-out-of-Learning-Applications-A-Survey-on-Automated-Machine-Learning/index.html">
<meta property="og:site_name" content="Ace">
<meta property="og:description" content="[TOC]
Automated Machine Learning
Key words: automatic machine learning, neural architecture search, hyper-parameter optimization, meta-learning, transfer-learning
1. IntroductionEvery aspect of machin">
<meta property="og:image" content="http://yoursite.com/img/AutoML-basic framework.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-working flow if approaches.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-Heuristic search.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-model based derivative free optimization.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-RL.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-Greedy search.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-Meta learning.png">
<meta property="og:image" content="http://yoursite.com/img/AutoML-transfer learning.png">
<meta property="og:updated_time" content="2019-06-11T08:01:23.105Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Taking Human out of Learning Applications: A Survey on Automated Machine Learning">
<meta name="twitter:description" content="[TOC]
Automated Machine Learning
Key words: automatic machine learning, neural architecture search, hyper-parameter optimization, meta-learning, transfer-learning
1. IntroductionEvery aspect of machin">
<meta name="twitter:image" content="http://yoursite.com/img/AutoML-basic framework.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/11/Taking-Human-out-of-Learning-Applications-A-Survey-on-Automated-Machine-Learning/"/>





  <title>Taking Human out of Learning Applications: A Survey on Automated Machine Learning | Ace</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ace</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Ace.github.io</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/11/Taking-Human-out-of-Learning-Applications-A-Survey-on-Automated-Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ace_ZAJ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/me.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ace">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Taking Human out of Learning Applications: A Survey on Automated Machine Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T11:29:15+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<p><strong>Automated Machine Learning</strong></p>
<p>Key words: automatic machine learning, neural architecture search, hyper-parameter optimization, meta-learning, transfer-learning</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Every aspect of machine learning applications, such as feature engineering, model selection, algorithm selection needs to be carefully configured, which is usually involved heavily with human experts.</p>
<p>AutoML attempts to reduce human assistance in the design, selection and implementation of various machine learning tools used in applications’ pipeline.</p>
<p>Examples:</p>
<ul>
<li>Auto-sklearn, try a collection of classifiers on a new problem, then get final predictions from an ensemble of them</li>
<li>Neural architecture search NAS. Success of AlexNet, VGGNet, GoogleNet, ResNet and DenseNet. Can the neural architecture be automatically designed so that good learning performance can be obtained on the given tasks. <strong>Reinforcement Learning</strong> has been a powerful and promising tool for NAS.</li>
<li>Automatic feature engineering, aims to construct a new features set, with which the performance of subsequent machine learning tools can be improved. Existing works on this topic include Data Science Machine(DSM), ExploreKit and FeatureHub</li>
</ul>
<a id="more"></a>
<h4 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h4><ul>
<li>Define the AutoML problem</li>
<li>Propose a general framework for existing AutoML approaches<ul>
<li>Setting up taxonomies of existing works</li>
<li>Gives insights of the problem existing approaches want to solve</li>
</ul>
</li>
<li>Categorize the existing AutoML works based on ‘what’ and ‘how’<ul>
<li>Problem setup is from ‘what’ perspective, which justifies which learning tools we want to make automated</li>
<li>Techniques are from ‘how’, they are methods to solve automl problems</li>
</ul>
</li>
<li>A detailed analysis of approaches techniques</li>
<li>Suggest four promising future  research directions: computational efficiency, problem settings, solution techniques and applications</li>
</ul>
<h3 id="2-Overview"><a href="#2-Overview" class="headerlink" title="2. Overview"></a>2. Overview</h3><h4 id="AutoML-from-two-perspectives"><a href="#AutoML-from-two-perspectives" class="headerlink" title="AutoML from two perspectives"></a>AutoML from two perspectives</h4><p>automation and machine learning</p>
<ul>
<li><p>ML</p>
<p><strong>E</strong>xperience, <strong>T</strong>ask, <strong>P</strong>erformance</p>
<p>AutoML emphasizes on how easy learning tools can be used</p>
</li>
<li><p>Automation</p>
<p>construct high-level controlling approaches over underneath learning tools so that proper configurations can be found without human assistance</p>
</li>
</ul>
<h4 id="Definition-of-AutoML"><a href="#Definition-of-AutoML" class="headerlink" title="Definition of AutoML"></a>Definition of AutoML</h4><p>$$ max_{configurations}$$performance of learning tools</p>
<p>s.t. no human assistance and limited computational budget</p>
<blockquote>
<p>AutoML attempts to construct machine learning programs (specified by E, T and P in machine learning definition), without human assistance and within limited computational budgets</p>
</blockquote>
<h4 id="Goals-of-AutoML"><a href="#Goals-of-AutoML" class="headerlink" title="Goals of AutoML"></a>Goals of AutoML</h4><p>Three core goals:</p>
<ul>
<li>Better performance: good generalization across various input data and learning tasks</li>
<li>No assistance from humans: configurations can be automatically done for machine learning tools</li>
<li>Lower computational budgets: the program can return an output within a limited budget</li>
</ul>
<h4 id="Basic-framework"><a href="#Basic-framework" class="headerlink" title="Basic framework"></a>Basic framework</h4><p>Proposed AutoML framework:</p>
<p><strong>Evaluator</strong>: The duty of the evaluator is to measure the performance of the learning tools with configurations provided by the optimizer. After that, it generates the feedback to the optimizer</p>
<p><strong>Optimizer</strong>: Update or generate the configuration for learning tools. The search space of the optimizer is defined by learning tools</p>
<p><img src="/img/AutoML-basic framework.png" alt="AutoML-basic framework"></p>
<h4 id="Taxonomies-of-AutoML-Approaches"><a href="#Taxonomies-of-AutoML-Approaches" class="headerlink" title="Taxonomies of AutoML Approaches"></a>Taxonomies of AutoML Approaches</h4><p>Based on what and how to automate</p>
<ol>
<li><p>‘What to automate’: by Problem Setup</p>
<ul>
<li>general case<ul>
<li>Feature Engineering </li>
<li>Model selection</li>
<li>Algorithm selection</li>
</ul>
</li>
<li>Deep learning<ul>
<li>Neural Architecture Search</li>
</ul>
</li>
</ul>
</li>
<li><p>‘How to automate’: by Techniques</p>
<ul>
<li><p>Basic</p>
<ul>
<li>Optimizer: Optimization &amp; Search<ul>
<li>Simple search approaches: grid search, random search</li>
<li>Optimization from samples</li>
<li>Gradient descent</li>
<li>More complex ones: reinforcement learning and automatic differentiation</li>
</ul>
</li>
<li>Evaluator: Performance Evaluation<ul>
<li>Basic evaluation strategies</li>
</ul>
</li>
</ul>
</li>
<li><p>Advanced</p>
<p>Experienced techniques learn and accumulate knowledge from past searches or external data. They usually need to be combined with basic ones</p>
<ul>
<li>Meta Learning</li>
<li>Transfer Learning</li>
</ul>
<p>They both try to make use of external knowledge to enhance basic ones for the optimizer and evaluator</p>
</li>
</ul>
</li>
</ol>
<p>We focus on supervised AutoML approaches in this survey as all existing works for AutoML are supervised ones </p>
<h4 id="Working-flow-based-on-Taxonomies"><a href="#Working-flow-based-on-Taxonomies" class="headerlink" title="Working flow based on Taxonomies"></a>Working flow based on Taxonomies</h4><p><img src="/img/AutoML-working flow if approaches.png" alt="AutoML-working flow if approaches.png"></p>
<h3 id="3-Problem-settings"><a href="#3-Problem-settings" class="headerlink" title="3. Problem settings"></a>3. Problem settings</h3><ul>
<li>What learning process we want to focus on?</li>
<li>What learning tools can be designed and used?</li>
<li>What are resultant corresponding configurations?</li>
</ul>
<p>By answering the questions we can define the search space  for an AutoML approach</p>
<h4 id="3-1-Feature-Engineering"><a href="#3-1-Feature-Engineering" class="headerlink" title="3.1 Feature Engineering"></a>3.1 Feature Engineering</h4><p>Automatically construct features from the data so that subsequent learning tools can have good performance</p>
<p>The above goal can be further divided into two sub-problems: </p>
<ul>
<li>creating features from the data</li>
<li>enhance features’ discriminative ability (For now, we focus on this)</li>
</ul>
<h5 id="Feature-Enhancing-Methods"><a href="#Feature-Enhancing-Methods" class="headerlink" title="Feature Enhancing Methods"></a>Feature Enhancing Methods</h5><ul>
<li><p><strong>Dimension reduction</strong></p>
<p>select a subset of features from the original ones, popular methods are greed search and lasso</p>
<p>Feature projection transforms original features to a new low-dimensional space, e.g., PCA, LDA, autoencoders</p>
</li>
<li><p><strong>Feature generation</strong></p>
<p>construct new features from the original ones based on some pre-defined operations.</p>
<p>e.g., multiplication of two features and standard normalization</p>
</li>
<li><p><strong>Feature encoding</strong></p>
<p>re-interprets original features based on some dictionaries learned from the data. Training samples that are not discriminable in the original space become separable in the new space.</p>
<p>e.g. sparse coding, local-linear coding, kernel methods(used with SVM)</p>
</li>
</ul>
<h5 id="Search-space"><a href="#Search-space" class="headerlink" title="Search space"></a>Search space</h5><ul>
<li><p><strong>hyper-parameters</strong> of tools, and configuration exactly refers to these hyper-parameters</p>
<p>e.g., determine the dimension of features when employing PCA, and the level of sparsity if sparse coding is used</p>
</li>
<li><p>contains <strong>features</strong> to be generated and selected, commonly considered in feature generation. Basically, the search space if spanned by operations on original features, from plus, minus and times operation</p>
</li>
</ul>
<h4 id="3-2-Model-selection"><a href="#3-2-Model-selection" class="headerlink" title="3.2 Model selection"></a>3.2 Model selection</h4><ul>
<li>pick up some classifiers</li>
<li>set their corresponding hyper-parameters</li>
</ul>
<h5 id="Classification-Tools"><a href="#Classification-Tools" class="headerlink" title="Classification Tools"></a>Classification Tools</h5><p>e.g.,</p>
<ul>
<li>tree classifiers</li>
<li>linear classifiers</li>
<li>kernel machines</li>
<li>deep networks</li>
</ul>
<h5 id="Search-space-1"><a href="#Search-space-1" class="headerlink" title="Search space"></a>Search space</h5><p>The candidate classifiers and their corresponding hyper-parameters make up the search space</p>
<h4 id="3-3-Optimization-Algorithm-Selection"><a href="#3-3-Optimization-Algorithm-Selection" class="headerlink" title="3.3 Optimization Algorithm Selection"></a>3.3 Optimization Algorithm Selection</h4><p>Automatically find an optimization algorithm so that <strong>efficiency</strong> and <strong>performance</strong> can be balanced</p>
<h5 id="Optimization-algorithms"><a href="#Optimization-algorithms" class="headerlink" title="Optimization algorithms"></a>Optimization algorithms</h5><ul>
<li>Gradient descent(GD), suffers from convergence and expensive per-iteration complexity.</li>
<li>L-BFGS, Limited memory-BFGS, more expensive but converges faster</li>
<li>SGD, stochastic gradient descent, each iteration is very cheap but many iterations are need before convergence</li>
</ul>
<h5 id="Search-space-2"><a href="#Search-space-2" class="headerlink" title="Search space"></a>Search space</h5><p>determined by configurations of optimization algorithms, contains the choice of optimization algorithms and the values of their hyper-parameters</p>
<h4 id="3-4-Full-scope"><a href="#3-4-Full-scope" class="headerlink" title="3.4 Full scope"></a>3.4 Full scope</h4><p>Two classes of full-scope AutoML approaches</p>
<ul>
<li><p>general case</p>
<p>combination of feature engineering, model selection and algorithm selection</p>
</li>
<li><p>NAS</p>
<p>target at searching good deep network architectures that suit learning problem</p>
<p>Three reasons why discuss it in parallel with the full scope</p>
<ul>
<li>hot research topic</li>
<li>application domain for deep networks is relative clear</li>
<li>domain specific network architectures can fulfill the learning purpose, where feature engineering and model selection are both done by NAS</li>
</ul>
</li>
</ul>
<h5 id="Network-Architecture-Search-NAS"><a href="#Network-Architecture-Search-NAS" class="headerlink" title="Network Architecture Search(NAS)"></a>Network Architecture Search(NAS)</h5><p>e.g. CNN</p>
<ul>
<li>number of filters </li>
<li>filter height</li>
<li>stride height</li>
<li>filter width</li>
<li>stride width</li>
<li>skip connections</li>
</ul>
<h3 id="4-Basic-techniques-for-optimizer"><a href="#4-Basic-techniques-for-optimizer" class="headerlink" title="4. Basic techniques for optimizer"></a>4. Basic techniques for optimizer</h3><p>After the search space is defined, we need to find an optimizer to guide the search in the space</p>
<p>Three important questions:</p>
<ul>
<li>What kind of <strong>search space</strong> can the optimizer operate on?</li>
<li>What kind of <strong>feedbacks</strong> it needs?</li>
<li>How many <strong>configurations</strong> it needs to generate/update before a good one can be found? – efficiency</li>
</ul>
<p>Divide techniques into three categories:</p>
<ul>
<li>simple search approaches</li>
<li>optimization from samples</li>
<li>gradient descent</li>
</ul>
<h4 id="4-1-Simple-search-approaches"><a href="#4-1-Simple-search-approaches" class="headerlink" title="4.1 Simple search approaches"></a>4.1 Simple search approaches</h4><p>make no assumptions about the search space</p>
<p>each configuration in the search space can be evaluated independently</p>
<ul>
<li><p><strong>Grid search(brute force)</strong></p>
<p>Enumerate every possible configurations in the search space</p>
<p>Discretization is necessary when the search space is continuous</p>
</li>
<li><p><strong>Random search</strong></p>
<p>Randomly samples configurations in the search space</p>
<p>Random search can explore more on important dimensions than grid search</p>
</li>
</ul>
<p>Simple search does not exploit the knowledge gained from the past evaluations, it is usually inefficient. </p>
<h4 id="4-2-Optimization-from-samples"><a href="#4-2-Optimization-from-samples" class="headerlink" title="4.2 Optimization from samples"></a>4.2 Optimization from samples</h4><p>iteratively generates new configurations based on previously samples</p>
<p>more efficient than simple search methods</p>
<p>does not make specific assumptions about the objective</p>
<p>divide into 3 categories:</p>
<ul>
<li>heuristic search</li>
<li>model-based derivative-free optimization</li>
<li>reinforcement learning</li>
</ul>
<h5 id="Heuristic-search-启发式搜索"><a href="#Heuristic-search-启发式搜索" class="headerlink" title="Heuristic search 启发式搜索"></a>Heuristic search 启发式搜索</h5><p>often inspired by biologic behaviors and phenomenon</p>
<p>widely used to solve optimization problems that are non-convex, non-smooth, or even non-continuous</p>
<p>population-based optimization methods</p>
<p><img src="/img/AutoML-Heuristic search.png" alt="AutoML-working flow if approaches.png"></p>
<p>popular heuristic search methods:</p>
<ul>
<li><p><strong>Popular swarm optimization(PSO)</strong> 粒子群优化</p>
<p>inspired by the behavior of biological communities that exhibit both individual and social behavior</p>
<p>the population is updated by moving towards the best individuals </p>
<p>PSO optimizes by searching the neighborhoods of the best samples</p>
<p>PSO has been used for model selection, feature selection for support vector machine (SVM), and hyper-parameter tuning for deep networks</p>
</li>
<li><p><strong>Evolutionary algorithms</strong></p>
<p>inspired by biological evolution</p>
<p>generation step contains crossover and mutation</p>
<ul>
<li><p><strong>crossover</strong> 交叉</p>
<p>involve two different individuals, combine them in some way to generate an new individual</p>
</li>
<li><p><strong>mutation</strong> 变异</p>
<p>slightly changes an individual to generate a new one</p>
</li>
</ul>
<p>With crossover mainly to exploit and mutation mainly to explore, the population is expected to evolve towards better performance.</p>
<p>Has been applied in feature selection and generation and model selection</p>
</li>
</ul>
<h5 id="Model-based-Derivative-Free-Optimization"><a href="#Model-based-Derivative-Free-Optimization" class="headerlink" title="Model-based Derivative-Free Optimization"></a>Model-based Derivative-Free Optimization</h5><p>Model-based optimization builds a model based on visited samples</p>
<p>Full utilization of feedbacks from the evaluator helps it <strong>generate more promising new samples</strong></p>
<p><img src="/img/AutoML-model based derivative free optimization.png" alt="AutoML-working flow if approaches.png"></p>
<p>Popular methods: </p>
<ul>
<li><p><strong>Bayesian optimization</strong></p>
<p>builds a probabilistic model</p>
<p>define an acquisition function based on the probabilistic model</p>
<p>a new sample is generated by optimizing the acquisition function and used to update the probabilistic model once its evaluated</p>
</li>
<li><p><strong>classification-based optimization(CBO)</strong></p>
<p>classification-based optimization learns a classifier that divides the search space into positive and negative areas. </p>
<p>Then, new samples are randomly generated in the positive area where it is more likely to get better configurations</p>
<p>efficient</p>
</li>
<li><p><strong>Simultaneous optimistic optimization(SOO)</strong></p>
<p>branch-and-bound optimization algorithm</p>
<p>tree structure is built on the search place</p>
<p>SOO deeply explores the search space by expanding leaf nodes</p>
<p>Through the tree model, SOO can balance exploration and exploitation to find the global optimum then the objective function is Local-Lipschitz continuous</p>
<p>suffer from curse of dimensionality</p>
</li>
</ul>
<h5 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h5><p>can solve problems with delayed feedbacks</p>
<p><img src="/img/AutoML-RL.png" alt="AutoML-working flow if approaches.png"></p>
<p>The feedbacks(reward and state) do not need to be immediately returned once an action is taken. They can be returned after performing a sequence of actions</p>
<p>RL is recently used in NAS. Design of one layer can be seen as one action given by the optimizer. Due to the delayed feedbacks, AutoML with reinforcement learning is highly source-consuming.</p>
<p>Some current endeavors addressing this problems are <strong>learning transferable architectures from smaller data sets,</strong> and <strong>cutting the search space by sharing parameter</strong> </p>
<p>RL has also been used for optimization algorithms search, automated feature selection  and training data selection in active learning</p>
<h4 id="4-3-Gradient-descent"><a href="#4-3-Gradient-descent" class="headerlink" title="4.3 Gradient descent"></a>4.3 Gradient descent</h4><p>Optimization problems of AutoML is very complex, and the objective is usually not differentiable or even not continuous.</p>
<p>In AutoML problem, the gradients need to be numerically computed</p>
<ol>
<li><p>This can be done with finite differentiation methods but at high costs</p>
<p>The computation of exact gradients relies on the convergence of model training</p>
<p>Through inexact gradient, hyper-parameters can be updated before the model training converges, which makes gradient descent method more efficient</p>
</li>
<li><p>Another way to compute gradients is through reversible learning (automatic differentiation)</p>
<p>compute gradients with chain rule</p>
<p>backpropagation process of network training</p>
</li>
</ol>
<h4 id="4-4-Greedy-search"><a href="#4-4-Greedy-search" class="headerlink" title="4.4 Greedy search"></a>4.4 Greedy search</h4><p>natural strategy to solve multi-step decision making problem</p>
<p>follows a heuristic that makes locally optimal decision at each step with the intent of finding a global optimum</p>
<p>applied in feature selection and submodular optimization</p>
<p>E.g., in NAS problem, greedy search is applied for multi-attribute learning problems</p>
<p>greedy search is also employed to search block structures within a cell, which is later used to construct a full CNN</p>
<p><img src="/img/AutoML-Greedy search.png" alt="AutoML-working flow if approaches.png"></p>
<h4 id="4-5-Other-techniques"><a href="#4-5-Other-techniques" class="headerlink" title="4.5 Other techniques"></a>4.5 Other techniques</h4><p>popular one is to change the landscape of the search space so that more powerful optimization techniques can be used</p>
<p>E.g., discrete search space to continuous, then use gd instead of RL. or encoder-decoder framework</p>
<h3 id="5-Basic-techniques-for-evaluator"><a href="#5-Basic-techniques-for-evaluator" class="headerlink" title="5. Basic techniques for evaluator"></a>5. Basic techniques for evaluator</h3><p>Very time-consuming as it involves model training for most of the times</p>
<p>Three important questions:</p>
<ul>
<li>Can the technique provide <strong>fast</strong> evaluation?</li>
<li>Can the technique provide <strong>accurate</strong> evaluation?</li>
<li>What <strong>feedbacks</strong> need to be provided by the evaluator?</li>
</ul>
<p>Tradeoff between the first and second question</p>
<p>Lower accuracy but larger variance</p>
<h4 id="5-1-Techniques"><a href="#5-1-Techniques" class="headerlink" title="5.1 Techniques"></a>5.1 Techniques</h4><h5 id="Direct-evaluation"><a href="#Direct-evaluation" class="headerlink" title="Direct evaluation"></a>Direct evaluation</h5><p>simplest method</p>
<p>the model parameters are learned on the training set, and the performance is measured on the validation set afterwards</p>
<p>accurate but expensive</p>
<h5 id="Sub-sampling"><a href="#Sub-sampling" class="headerlink" title="Sub-sampling"></a>Sub-sampling</h5><p>training time depends heavily on the amount of training data</p>
<p>train parameters with a subset of the training data</p>
<p>either using a subset of samples, a subset of features or multi-fidelity(多高保真) evaluations</p>
<h5 id="Early-stop"><a href="#Early-stop" class="headerlink" title="Early stop"></a>Early stop</h5><p>it is usually used to cut down the training time for unpromising configurations</p>
<p>If a poor early-stage performance is observed, the evaluator can terminate the training and report a low performance to indicate that the candidate is unpromising</p>
<p>introduces noise and bias to the estimation, maybe some features eventually turn out to be good after sufficient training</p>
<h5 id="Parameter-reusing"><a href="#Parameter-reusing" class="headerlink" title="Parameter reusing"></a>Parameter reusing</h5><p>use parameters of the trained models in previous evaluation</p>
<p>to warm-start the model training for the current evaluation</p>
<p>parameters learned with similar configurations can be close with each other</p>
<p>as different start point may lead convergence to different local optima, it sometimes brings bias in the evaluation</p>
<p>one of the most straightforward applications of transfer learning</p>
<h5 id="Surrogate-evaluator"><a href="#Surrogate-evaluator" class="headerlink" title="Surrogate evaluator"></a>Surrogate evaluator</h5><p>cut down the evaluation cost is to build a model that predicts the performance of given configurations, with experience of past evaluations</p>
<p>serving as surrogate evaluators, sparse the computationally expensive model training, significantly accelerate AutoML</p>
<p>the application scope is limited to hyper-parameter optimization since other kinds of configurations are often hard to quantize, meta-learning are promising to address this problem</p>
<h3 id="6-Experienced-Techniques"><a href="#6-Experienced-Techniques" class="headerlink" title="6. Experienced Techniques"></a>6. Experienced Techniques</h3><p>experienced techniques that can improve the efficiency and performance of AutoML</p>
<ul>
<li><p>meta-learning</p>
<p>meta-knowledge about learning is extracted</p>
<p>meta-learner is trained to guide learning</p>
</li>
<li><p>transfer learning</p>
<p>transferable knowledge is brought from past experiences to help upcoming learning</p>
</li>
</ul>
<p>They all aim to exploit experience gained from past learning practices</p>
<p>Some researchers also take transfer learning as a special case of meta-learning</p>
<h4 id="6-1-Meta-learning"><a href="#6-1-Meta-learning" class="headerlink" title="6.1 Meta-learning"></a>6.1 Meta-learning</h4><p>Categorizing them into three general classes:</p>
<ul>
<li>for the evaluator</li>
<li>for the optimizer</li>
<li>for dynamic configuration adaptation</li>
</ul>
<h5 id="General-Meta-Learning-Framework"><a href="#General-Meta-Learning-Framework" class="headerlink" title="General Meta-Learning Framework"></a>General Meta-Learning Framework</h5><p><img src="/img/AutoML-Meta learning.png" alt="AutoML-working flow if approaches.png"></p>
<h5 id="Configuration-Evaluation"><a href="#Configuration-Evaluation" class="headerlink" title="Configuration Evaluation"></a>Configuration Evaluation</h5><p>Most computation-intensive step: configuration evaluation, due to cost of model training and validation</p>
<p>Categorize：</p>
<ul>
<li><p><strong>Model evaluation</strong></p>
<p>meta knowledge: meta features of learning problems and the empirical performance of different models</p>
<p>meta learner: map the meta-features to the performance, applicability, ranking of models</p>
</li>
<li><p><strong>General configuration evaluation</strong></p>
<p>E.g., in ExploreKit, ranking classifiers are trained to candidate features</p>
</li>
</ul>
<p>Meta-learners are trained to predict the performance or suitability of configurations</p>
<p>where all possible choices have been enumerated, best configurations can be directly selected according to the scores and rankings predicted by the meta-learner</p>
<h5 id="Configuration-Generation"><a href="#Configuration-Generation" class="headerlink" title="Configuration Generation"></a>Configuration Generation</h5><p>Meta-Learning can also facilitate configuration generation by learning</p>
<ul>
<li><p>Promising configuration generation</p>
<p>meta-knowledge indicate the empirically good configurations are extracted</p>
<p>meta-learner, take learning problem as input and predict promising configurations</p>
<p>it is also possible to learn promising configuration generation strategies(e.g., feature transformations)</p>
</li>
<li><p>Warm-starting configuration generation</p>
<p>better initialize configuration search</p>
<p>given a new learning task, to identify the past tasks that are closest to it in the meta- feature space, and use their best-performing configurations to initialize search</p>
</li>
<li><p>Search space refining</p>
<p>make effort to evaluate the importance of configurations, or identify promising regions in the search space</p>
</li>
</ul>
<h5 id="Dynamic-Configuration-Adaptation"><a href="#Dynamic-Configuration-Adaptation" class="headerlink" title="Dynamic Configuration Adaptation"></a>Dynamic Configuration Adaptation</h5><p>the data distribution varies even in a single data set, especially in data streams</p>
<p><strong>concept drift</strong>: such change in data distribution</p>
<p>Classical ml, concept drift is often priorly assumed or posteriorly detected</p>
<p>Meta-learning can help to automate this procedure by detecting concept drift and dynamically adapting learning tools</p>
<ul>
<li><p>Concept drift detection</p>
<p>some papers…</p>
</li>
<li><p>Dynamic configuration adaptation</p>
<p>Once the concept drift is detected, configuration adaptation can be carried out </p>
<p>similar in promising configuration generation</p>
</li>
</ul>
<h4 id="6-2-Transfer-learning"><a href="#6-2-Transfer-learning" class="headerlink" title="6.2 Transfer learning"></a>6.2 Transfer learning</h4><p>By using the knowledge from the source domain and learning task</p>
<p><img src="/img/AutoML-transfer learning.png" alt="AutoML-working flow if approaches.png"></p>
<ul>
<li>configuration generation(for the optimizer)</li>
<li>configuration evaluation(for the evaluator)</li>
</ul>
<h5 id="Configuration-Generation-1"><a href="#Configuration-Generation-1" class="headerlink" title="Configuration Generation"></a>Configuration Generation</h5><p>reuse trained surrogate models or promising search strategies from past AutoML search(source) and improve the efficiency in current AutoML task(target)</p>
<ul>
<li><p>surrogate model transfer</p>
<p>transfer the surrogate model, or its components such as kernel function</p>
<p>general case is hyper-parameter optimization machine</p>
</li>
<li><p>network cell transfer</p>
</li>
</ul>
<p>Multi-task learning, is also employed to help configuration generation</p>
<h5 id="Configuration-Evaluation-1"><a href="#Configuration-Evaluation-1" class="headerlink" title="Configuration Evaluation"></a>Configuration Evaluation</h5><p>transfer knowledge from previous configuration evaluations</p>
<p>widely employed in NAS approaches to accelerate  the evaluation of candidate architectures</p>
<ul>
<li><p>Model parameter transfer</p>
<p>initialize network with transferred feature layers, followed by fine-tuning</p>
</li>
<li><p>Function preserving transformation</p>
<p>first proposed in Net2Net, where new networks are initialized to represent the same functionality of a given trained model</p>
<p>also inspires new strategies to explore the network architecture space</p>
</li>
</ul>
<h3 id="7-Representative-examples"><a href="#7-Representative-examples" class="headerlink" title="7. Representative examples"></a>7. Representative examples</h3><h4 id="7-1-Model-selection-using-Auto-sklearn"><a href="#7-1-Model-selection-using-Auto-sklearn" class="headerlink" title="7.1 Model selection using Auto-sklearn"></a>7.1 Model selection using Auto-sklearn</h4><p>CASH problem</p>
<p>aim to minimize the validation loss with respect to the model as well as its hyper-parameters and parameters</p>
<h4 id="7-2-Reinforcement-learning-for-NAS-NASNet"><a href="#7-2-Reinforcement-learning-for-NAS-NASNet" class="headerlink" title="7.2 Reinforcement learning for NAS(NASNet)"></a>7.2 Reinforcement learning for NAS(NASNet)</h4><p>RL</p>
<p>transfer learning </p>
<p>parameter sharing</p>
<p>greedy search</p>
<p>CNN with less depth and fewer parameters with comparable performance to that of state- of-art CNN designed by humans.</p>
<h4 id="7-3-Feature-Construction-using-ExploreKit"><a href="#7-3-Feature-Construction-using-ExploreKit" class="headerlink" title="7.3 Feature Construction using ExploreKit"></a>7.3 Feature Construction using ExploreKit</h4><ul>
<li><p>candidate feature generation</p>
</li>
<li><p>candidate feature ranking</p>
<p>meta learning employed to accelerate</p>
<p>with historical knowledge on feature engineering     </p>
</li>
<li><p>candidate feature evaluation and selection</p>
</li>
</ul>
<h3 id="8-Summary"><a href="#8-Summary" class="headerlink" title="8. Summary"></a>8. Summary</h3><h4 id="8-1-A-brief-history"><a href="#8-1-A-brief-history" class="headerlink" title="8.1 A brief history"></a>8.1 A brief history</h4><h4 id="8-2-Current-status-in-the-academy-and-industry"><a href="#8-2-Current-status-in-the-academy-and-industry" class="headerlink" title="8.2 Current status in the academy and industry"></a>8.2 Current status in the academy and industry</h4><p>ICML NIPS KDD AAAI IJCAI JMLR</p>
<h4 id="8-3-Future-works"><a href="#8-3-Future-works" class="headerlink" title="8.3 Future works"></a>8.3 Future works</h4><h5 id="Problem-setup"><a href="#Problem-setup" class="headerlink" title="Problem setup"></a>Problem setup</h5><p>automatically creating features from the data</p>
<h5 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a>Techniques</h5><ul>
<li><p>Basic techniques</p>
<p>higher efficiency</p>
<p>simultaneously optimize configurations and parameters</p>
</li>
<li><p>Experienced techniques</p>
<ul>
<li><p>meta learning</p>
<p>how to automate meta-learning..</p>
</li>
<li><p>transfer learning</p>
<p>automatically determine when and how to transfer what knowledge</p>
</li>
</ul>
</li>
<li><p>Applications</p>
<ul>
<li>active learning</li>
<li>neural network compression</li>
<li>semi-supervised learning</li>
</ul>
</li>
</ul>
<h5 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h5><ul>
<li><p>Optimization theory</p>
<p>convergence speed</p>
</li>
<li><p>Learning theory</p>
<p>which type of problems can or cannot be addressed by an AutoML approach</p>
</li>
</ul>
<p>Clarify AutoML approach generalization ability</p>
<p>AdaNet</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/AutoML/" rel="tag"># AutoML</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/09/Rakiki-Machine-Learning-as-an-Analytics-Service-System/" rel="next" title="Rakiki--Machine Learning as an Analytics Service System">
                <i class="fa fa-chevron-left"></i> Rakiki--Machine Learning as an Analytics Service System
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/18/112-Path-Sum/" rel="prev" title="112.Path Sum">
                112.Path Sum <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/me.JPG"
                alt="Ace_ZAJ" />
            
              <p class="site-author-name" itemprop="name">Ace_ZAJ</p>
              <p class="site-description motion-element" itemprop="description">boom pow</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">127</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-number">1.</span> <span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Contributions"><span class="nav-number">1.1.</span> <span class="nav-text">Contributions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Overview"><span class="nav-number">2.</span> <span class="nav-text">2. Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AutoML-from-two-perspectives"><span class="nav-number">2.1.</span> <span class="nav-text">AutoML from two perspectives</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Definition-of-AutoML"><span class="nav-number">2.2.</span> <span class="nav-text">Definition of AutoML</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Goals-of-AutoML"><span class="nav-number">2.3.</span> <span class="nav-text">Goals of AutoML</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-framework"><span class="nav-number">2.4.</span> <span class="nav-text">Basic framework</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Taxonomies-of-AutoML-Approaches"><span class="nav-number">2.5.</span> <span class="nav-text">Taxonomies of AutoML Approaches</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Working-flow-based-on-Taxonomies"><span class="nav-number">2.6.</span> <span class="nav-text">Working flow based on Taxonomies</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Problem-settings"><span class="nav-number">3.</span> <span class="nav-text">3. Problem settings</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Feature-Engineering"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Feature Engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Feature-Enhancing-Methods"><span class="nav-number">3.1.1.</span> <span class="nav-text">Feature Enhancing Methods</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Search-space"><span class="nav-number">3.1.2.</span> <span class="nav-text">Search space</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Model-selection"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Model selection</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Classification-Tools"><span class="nav-number">3.2.1.</span> <span class="nav-text">Classification Tools</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Search-space-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">Search space</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Optimization-Algorithm-Selection"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Optimization Algorithm Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Optimization-algorithms"><span class="nav-number">3.3.1.</span> <span class="nav-text">Optimization algorithms</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Search-space-2"><span class="nav-number">3.3.2.</span> <span class="nav-text">Search space</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-Full-scope"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 Full scope</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Network-Architecture-Search-NAS"><span class="nav-number">3.4.1.</span> <span class="nav-text">Network Architecture Search(NAS)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Basic-techniques-for-optimizer"><span class="nav-number">4.</span> <span class="nav-text">4. Basic techniques for optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Simple-search-approaches"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Simple search approaches</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Optimization-from-samples"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Optimization from samples</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Heuristic-search-启发式搜索"><span class="nav-number">4.2.1.</span> <span class="nav-text">Heuristic search 启发式搜索</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Model-based-Derivative-Free-Optimization"><span class="nav-number">4.2.2.</span> <span class="nav-text">Model-based Derivative-Free Optimization</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reinforcement-learning"><span class="nav-number">4.2.3.</span> <span class="nav-text">Reinforcement learning</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Gradient-descent"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 Gradient descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-Greedy-search"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 Greedy search</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-Other-techniques"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 Other techniques</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Basic-techniques-for-evaluator"><span class="nav-number">5.</span> <span class="nav-text">5. Basic techniques for evaluator</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Techniques"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Direct-evaluation"><span class="nav-number">5.1.1.</span> <span class="nav-text">Direct evaluation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sub-sampling"><span class="nav-number">5.1.2.</span> <span class="nav-text">Sub-sampling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Early-stop"><span class="nav-number">5.1.3.</span> <span class="nav-text">Early stop</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Parameter-reusing"><span class="nav-number">5.1.4.</span> <span class="nav-text">Parameter reusing</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Surrogate-evaluator"><span class="nav-number">5.1.5.</span> <span class="nav-text">Surrogate evaluator</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Experienced-Techniques"><span class="nav-number">6.</span> <span class="nav-text">6. Experienced Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-Meta-learning"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 Meta-learning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#General-Meta-Learning-Framework"><span class="nav-number">6.1.1.</span> <span class="nav-text">General Meta-Learning Framework</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Configuration-Evaluation"><span class="nav-number">6.1.2.</span> <span class="nav-text">Configuration Evaluation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Configuration-Generation"><span class="nav-number">6.1.3.</span> <span class="nav-text">Configuration Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dynamic-Configuration-Adaptation"><span class="nav-number">6.1.4.</span> <span class="nav-text">Dynamic Configuration Adaptation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-Transfer-learning"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 Transfer learning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Configuration-Generation-1"><span class="nav-number">6.2.1.</span> <span class="nav-text">Configuration Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Configuration-Evaluation-1"><span class="nav-number">6.2.2.</span> <span class="nav-text">Configuration Evaluation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Representative-examples"><span class="nav-number">7.</span> <span class="nav-text">7. Representative examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-Model-selection-using-Auto-sklearn"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 Model selection using Auto-sklearn</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-Reinforcement-learning-for-NAS-NASNet"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 Reinforcement learning for NAS(NASNet)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-Feature-Construction-using-ExploreKit"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 Feature Construction using ExploreKit</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Summary"><span class="nav-number">8.</span> <span class="nav-text">8. Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-A-brief-history"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 A brief history</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-Current-status-in-the-academy-and-industry"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 Current status in the academy and industry</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-Future-works"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 Future works</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Problem-setup"><span class="nav-number">8.3.1.</span> <span class="nav-text">Problem setup</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Techniques"><span class="nav-number">8.3.2.</span> <span class="nav-text">Techniques</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Theory"><span class="nav-number">8.3.3.</span> <span class="nav-text">Theory</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ace_ZAJ</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.staticfile.org/MathJax/MathJax-2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

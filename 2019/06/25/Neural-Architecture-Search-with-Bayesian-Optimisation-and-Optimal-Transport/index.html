<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="default">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Neural Architecture Search with Bayesian Optimisation and Optimal Transport
AbstractBayesian Optimisation refers to a class of methods for global optimisation of a function $f$ which is only accessibl">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Architecture Search with Bayesian Optimisation and Optimal Transport">
<meta property="og:url" content="http://yoursite.com/2019/06/25/Neural-Architecture-Search-with-Bayesian-Optimisation-and-Optimal-Transport/index.html">
<meta property="og:site_name" content="Anne">
<meta property="og:description" content="Neural Architecture Search with Bayesian Optimisation and Optimal Transport
AbstractBayesian Optimisation refers to a class of methods for global optimisation of a function $f$ which is only accessibl">
<meta property="og:updated_time" content="2019-06-27T04:06:52.674Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Architecture Search with Bayesian Optimisation and Optimal Transport">
<meta name="twitter:description" content="Neural Architecture Search with Bayesian Optimisation and Optimal Transport
AbstractBayesian Optimisation refers to a class of methods for global optimisation of a function $f$ which is only accessibl">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/25/Neural-Architecture-Search-with-Bayesian-Optimisation-and-Optimal-Transport/"/>





  <title>Neural Architecture Search with Bayesian Optimisation and Optimal Transport | Anne</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Anne</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Anne.github.io</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/25/Neural-Architecture-Search-with-Bayesian-Optimisation-and-Optimal-Transport/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Anne_ZAJ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/me.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Anne">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Neural Architecture Search with Bayesian Optimisation and Optimal Transport</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-25T11:14:51+08:00">
                2019-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/AutoML/" itemprop="url" rel="index">
                    <span itemprop="name">AutoML</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>Neural Architecture Search with Bayesian Optimisation and Optimal Transport</strong></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Bayesian Optimisation refers to a class of methods for global optimisation of a function $f$ which is only accessible via point evaluations</p>
<p>Typically used in setting where $f$ is expensive to evaluate</p>
<p>Common use case for BO in ml is model selection</p>
<p>Conventional BO methods have focused on <strong>Euclidean and categorical domains</strong>, in the context of model selection, only <strong>permits tuning scalar hyper-parameters of machine learning algorithms</strong></p>
<p>NASBOT, a <strong>Gaussian process</strong> based on BO framework for NAS</p>
<p>Develop a <strong>distance metric</strong> in the space of neural network architecture which can be computed efficiently via an <strong>optimal transport program</strong></p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A noisy black-box function $f$ with the goal of finding its optimum in some domain $X$</p>
<p>BO uses Bayesian model for $f$ to <strong>infer function values at unexplored regions</strong> and <strong>guide the selection of points for future evaluations</strong></p>
<p>Quintessential use case for BO in ml is model selection. E.g., select the regularisation parameter $\lambda$ and kernel width $h$ for SVM. We can set this up as a zeroth order optimisation problem where our domain is a two dimensional space of $(\lambda, h)$ values, and each function evaluation trains the SVM on a training set, and computes the accuracy on a validation set.</p>
<p><strong>A most unadorned form</strong>:</p>
<ul>
<li>starting at time 0 with a GP prior for $f$</li>
<li>at time $t$, it incorporates results of evaluations from $1,…t-1$ in the form of a posterior for $f$</li>
<li>Then uses this posterior to construct an acquisition function\kappa\left(x, x^{\prime}\right), where $\varphi_t$ is a measure of the value of evaluating $f$ at $x$ at time $t$ if our goal is to maximize $f$</li>
</ul>
<p>It chooses to evaluate $f$ at the maximiser of the acquisition, $x_{t}=\operatorname{argmax}_{x \in \mathcal{X}} \varphi_{t}(x)$</p>
<p><strong>Two key ingredients:</strong></p>
<ul>
<li><p>quantify the similarity between two points $x, x’$ in the domain in the form of a kernel $\kappa\left(x, x^{\prime}\right)$ , allow us to reason about an unevaluated values $（x’)$ when we have already evaluated $f(x)$</p>
<p>in Euclidean spaces, we can use: Gaussian, Laplacian, Matern</p>
</li>
<li><p>a  method to maximize $\varphi_t(x)$</p>
<p>via off shelf branch-and-bound or gradient based methods</p>
</li>
</ul>
<p>$x$ is a neural network is not the case</p>
<ul>
<li>Quantify (dis)similarity between two networks</li>
<li>Traverse the space of such networks to optimise the acquisition function</li>
</ul>
<p><strong>Contributions:</strong></p>
<ul>
<li>Develop a (pseudo-)distance for neural network architectures called <strong>OTMANN</strong>(Optimal Transport Metrics for Architectures of Neural Networks) that can be computed efficiently via an <strong>optimal transport program</strong></li>
<li>BO framework for <strong>optimising functions on neural network</strong> architectures called <strong>NASBOT</strong>(Neural Architecture Search with Bayesian Optimisation and Optimal Transport). Include an <strong>evolutionary algorithm to optimise the acquisition</strong></li>
<li>NASBOT outperforms other baselines on model selection tasks for MLP and conv </li>
</ul>
<p><strong>Related Work:</strong></p>
<ul>
<li><p>evolutionary algorithm</p>
<ul>
<li><p>provide a simple mechanism to explore the space of architectures by making a sequence of changes to networks that have already been evaluated</p>
</li>
<li><p>not ideally suited for optimising functions that are expensive to evaluate</p>
</li>
</ul>
</li>
<li><p>rl</p>
<ul>
<li>no explicit need to maintain a notion of state and solve credit assignment</li>
<li>fundamentally more difficult problem than optimisation</li>
</ul>
</li>
</ul>
<h2 id="Set-up"><a href="#Set-up" class="headerlink" title="Set up"></a>Set up</h2><p>$f$ is the performance on a validation set after $x$ is trained on the training set. If $x_{\star}=\operatorname{argmax}_{\mathcal{X}} f(x)$, $x_t$ is the architecture evaluated at time $t$, we want $f\left(x_{\star}\right)-\max _{t \leq n} f\left(x_{t}\right)$ to vanish fast as the number of evaluations $n \rightarrow \infty$</p>
<h3 id="A-brief-review-of-Gaussian-Process-based-on-Bayesian-Optimisation"><a href="#A-brief-review-of-Gaussian-Process-based-on-Bayesian-Optimisation" class="headerlink" title="A brief review of Gaussian Process based on Bayesian Optimisation"></a>A brief review of Gaussian Process based on Bayesian Optimisation</h3><p>GP: </p>
<ul>
<li>mean function $\mu : \mathcal{X} \rightarrow \mathbb{R}$ </li>
<li>(covariance) kernel function: $\kappa : \mathcal{X}^{2} \rightarrow \mathbb{R}$</li>
</ul>
<p>Given $n$ observations $\mathcal{D}_{n}=\left{\left(x_{i}, y_{i}\right)\right}_{i=1}^{n}$, where $x_{i} \in \mathcal{X}, y_{i}=f\left(x_{i}\right)+\epsilon_{i} \in \mathbb{R}$, and $\epsilon_{i} \sim \mathcal{N}\left(0, \eta^{2}\right)$, the posterior process $f|D_n$ is also a GP with mean $\mu_n$, covariance $\kappa_n$<br>$$<br>k_{i}=\kappa\left(x, x_{i}\right), k_{i}^{\prime}=\kappa\left(x^{\prime}, x_{i}\right)<br>$$</p>
<p>$$<br>K_{i, j}=\kappa\left(x_{i}, x_{j}\right)<br>$$</p>
<p>Then, $\mu_{n}, \kappa_{n}$ can be computed via:<br>$$<br>\mu_{n}(x)=k^{\top}\left(K+\eta^{2} I\right)^{-1} Y\\<br>\quad \kappa_{n}\left(x, x^{\prime}\right)=\kappa\left(x, x^{\prime}\right)-k^{\top}\left(K+\eta^{2} I\right)^{-1} k^{\prime}<br>$$</p>
<p>At time $t$, we have already evaluated $f$ at points $\left{x_{i}\right}_{i=1}^{t-1}$ and obtained observations $\left{y_{i}\right}_{i=1}^{t-1}$  </p>
<p>To determine  the next point for evaluation $x_t$, we first use the posterior GP to define an acquisition function $\varphi_t$, which measure the utility of evaluating $f$ at any $x$ according to the posterior</p>
<p>Then maximize the acquisition function: $x_{t}=\operatorname{argmax}_{\mathcal{X}} \varphi_{t}(x)$, and evaluate $f$ at $x_t$</p>
<p><strong>Expected improvement acquisition</strong>:<br>$$<br>\varphi_{t}(x)=\mathbb{E}\left[\max \left{0, f(x)-\tau_{t-1}\right} |\left{\left(x_{i}, y_{i}\right)\right}_{i=1}^{t-1}\right]<br>$$<br>measures the expected improvement over the current maximum value  according to the posterior GP<br>$$<br>\tau_{t-1}=\operatorname{argmax}_{i&lt;t-1} f\left(x_{i}\right)<br>$$<br>$\tau_{t-1}$ denote the current best value</p>
<p><strong>GP/BO in the context of architecture search</strong></p>
<p>$\kappa\left(x, x^{\prime}\right)$ is a measure of similarity between $x$ and $x’$</p>
<p>If $\kappa\left(x, x^{\prime}\right)$ is large, then $f(x)$ and $f(x’)$ are highly corrlated</p>
<p>In BO, we select the next point, balance between <strong>exploitation</strong>, choosing points that we believe will have high $f$ value, and <strong>exploration</strong>, choosing points that we do not know much about so that we do not get such at a bad optimum</p>
<h3 id="A-mathematical-Formalism-for-Neural-networks"><a href="#A-mathematical-Formalism-for-Neural-networks" class="headerlink" title="A mathematical Formalism for Neural networks"></a>A mathematical Formalism for Neural networks</h3><p>View a neural network as a graph whose <strong>vertices</strong> are the <strong>layers</strong> of the network</p>
<p>$\mathcal{G}=(\mathcal{L}, \mathcal{E})$, a set of layers $\mathcal{L}$ and directed edges $\mathcal{E}$</p>
<p>An edge $(u, v) \in \mathcal{E}$ is a ordered pair of layers</p>
<p>A <strong>layer label $\ell \ell(u)$</strong> denotes the <strong>type of operations</strong> performed at the layer. $\ell_u()$ denotes the number of computational units in a layer</p>
<p>Each network has <strong>decision layers</strong> which are used to obtain the predictions of the network</p>
<ul>
<li>input layer  $\ell \ell(u_{ip})=ip$</li>
<li>output layer $\ell \ell(u_{op})=op$</li>
<li>processing layers</li>
<li>decision layer</li>
</ul>
<p>The output of each layer if fed to each of its children. When a layer has multiple parents, the inputs are concatenated</p>
<p>The neural network are also characterized by the values of the weights/parameters between layers. In AS, do not consider these weights ===?===</p>
<p> <strong>Distance</strong>:</p>
<p>Our eventual goal is a kernel for GP</p>
<p>A distance $d$, we will aim for $\kappa\left(x, x^{\prime}\right)=e^{-\beta d\left(x, x^{\prime}\right)^{p}}$</p>
<p>E.g.,$d$ is the L2 norm, p=1,2 correspond to the Laplacian and Gaussian kernels respectively</p>
<h2 id="The-OTMANN-distance"><a href="#The-OTMANN-distance" class="headerlink" title="The OTMANN distance"></a>The OTMANN distance</h2><p>OTAMNN is defined as the minimum of a matching scheme which attempts to <strong>match the computation at the layers of one network to the layers of the other</strong></p>
<p><strong>Penalties</strong> for matching layers with different types of operations or those at structurally different positions. Will find a <strong>matching that minimizes these penalties</strong></p>
<p>First two concepts: layer masses, path length</p>
<ul>
<li>layer masses</li>
<li>path length</li>
</ul>
<h3 id="layer-masses"><a href="#layer-masses" class="headerlink" title="layer masses"></a>layer masses</h3><p>$\ell m$, the quantity that match between the layers of two networks when comparing them</p>
<p>$\ell m(u)$, quantify the significance of layer $u$ </p>
<ul>
<li>For processing layer, computed via the product of $\ell u(u)$ and the number of incoming units</li>
<li>For input and output layer, use $\ell m\left(u_{\mathrm{ip}}\right)=\ell m\left(u_{\mathrm{op}}\right)=\zeta \sum_{u \in \mathcal{P} \mathcal{L}} \ell m(u)$where $\mathcal{P} \mathcal{L}$ denotes the set of processing layers, $\zeta$ is a parameter to be determined</li>
<li>For decision layer, assign an amount of mass proportional to the mass in the processing layers. Since, the outputs of the decision layers are averaged, we distribute the mass among all the decision layers $\forall u \in \mathcal{D} \mathcal{L}, \ell m(u)=\frac{\zeta}{|\mathcal{D} \mathcal{L}|} \sum_{u \in \mathcal{P} \mathcal{L}} \ell m(u)$, $\zeta=0.1$ </li>
</ul>
<h3 id="Path-lengths-from-to-u-ip-u-op"><a href="#Path-lengths-from-to-u-ip-u-op" class="headerlink" title="Path lengths from/to $u_{ip}, u_{op}$"></a>Path lengths from/to $u_{ip}, u_{op}$</h3><ul>
<li>shortest path length from $u$ to $v$</li>
<li>longest path length from $u$ to $v$</li>
<li>random walk path length, choose uniformly at random</li>
</ul>
<p>$\delta_{\mathrm{ip}}^{\mathrm{sp}}(u), \delta_{\mathrm{ip}}^{\operatorname{lp}}(u), \delta_{\mathrm{ip}}^{\mathrm{rw}}(u)$, walks from the input $u_{ip}$ to $u$</p>
<p>$Z(i, j)$ denotes the amount of mass matched between layer $i \in \mathcal{G}_{1}$ and $j \in \mathcal{G}_{2}$</p>
<ul>
<li>Label mismatch penalty $\phi_{lmm}$</li>
<li>Non-assignment penalty $\phi_{nas}$</li>
<li>Structural penalty $\phi_{str}$</li>
</ul>
<h4 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem 1"></a>Theorem 1</h4><p>===some math===</p>
<p>$d$ puts more emphasis on the amount of computation at the layers over structure</p>
<p>vice versa for $\overline d$</p>
<h2 id="NASBOT"><a href="#NASBOT" class="headerlink" title="NASBOT"></a>NASBOT</h2><p><strong>kernel function</strong>:<br>$$<br>\kappa = \alpha e^{-\beta d}+\overline{\alpha} d^{-\overline{\beta}} \overline{d}<br>$$<br>did not encounter an distance where the eigenvalues of the kernel matrix were negative. There are several methods to circumvent this issue in kernel methods</p>
<p>Use Evolutionary algorithm(EA) approach to optimize the acquisition function</p>
<p>Begin with an initial pool of networks and <strong>evaluate the acquisition $\varphi_t$ on those networks</strong></p>
<p>Then generate a set of $N_{num}$ mutations of this pool</p>
<ul>
<li>stochastically <strong>select</strong> $N_{mut}$ candidates from the set of network already evaluated such that those with higher $\varphi_t$ values are more likely to be selected than those with low values</li>
<li><strong>modify</strong> each candidate, to produce a new architecture<ul>
<li>increase or decrease the number of computational units of a layer</li>
<li>add or delete layers</li>
<li>change the connectivity of existing layers</li>
</ul>
</li>
<li>evaluate the acquisition on the $N_{mut}$ mutations, add it to the initial pool, and repeat for the prescribed number of steps</li>
</ul>
<p>EA works fine for cheap function, such as $\varphi_t$ which is analytically available,  it is not suitable when evaluations are expensive</p>
<p>If wishes to tune:</p>
<ul>
<li>drop-out probabilities, regularization penalties and batch normalization: treated as part of the layer label, via an augmented label penalty matrix $M$ which accounts for these considerations</li>
<li>scalar hyper-parameters(e.g., learning-rate): use an existing kernel for euclidean spaces and define the GP over the joint architecture + hyper-parameter space via a <strong>product kernel</strong></li>
<li>early stopping: incorporated by defining a fidelity space</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p>asynchronously parallel set up of 2-4GPUS</p>
<p>evaluate multiple models in <strong>parallel</strong>, with each model on a single GPU</p>
<p>also illustrate some of the best architectures found, on many datasets, common features were long <strong>skip connections</strong> and <strong>multiple decision layers</strong></p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/22/Large-Scale-Evolution-of-Image-Classifiers/" rel="next" title="Large-Scale Evolution of Image Classifiers">
                <i class="fa fa-chevron-left"></i> Large-Scale Evolution of Image Classifiers
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/27/Designing-neural-network-architectures-using-Reinforcement-learning/" rel="prev" title="Designing neural network architectures using Reinforcement learning">
                Designing neural network architectures using Reinforcement learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/me.JPG"
                alt="Anne_ZAJ" />
            
              <p class="site-author-name" itemprop="name">Anne_ZAJ</p>
              <p class="site-description motion-element" itemprop="description">boom pow</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">133</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Set-up"><span class="nav-number">3.</span> <span class="nav-text">Set up</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-brief-review-of-Gaussian-Process-based-on-Bayesian-Optimisation"><span class="nav-number">3.1.</span> <span class="nav-text">A brief review of Gaussian Process based on Bayesian Optimisation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-mathematical-Formalism-for-Neural-networks"><span class="nav-number">3.2.</span> <span class="nav-text">A mathematical Formalism for Neural networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-OTMANN-distance"><span class="nav-number">4.</span> <span class="nav-text">The OTMANN distance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#layer-masses"><span class="nav-number">4.1.</span> <span class="nav-text">layer masses</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Path-lengths-from-to-u-ip-u-op"><span class="nav-number">4.2.</span> <span class="nav-text">Path lengths from/to $u_{ip}, u_{op}$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">Theorem 1</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NASBOT"><span class="nav-number">5.</span> <span class="nav-text">NASBOT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-number">6.</span> <span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset"><span class="nav-number">6.1.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experimental-setup"><span class="nav-number">6.2.</span> <span class="nav-text">Experimental setup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Results"><span class="nav-number">6.3.</span> <span class="nav-text">Results</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Anne_ZAJ</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.staticfile.org/MathJax/MathJax-2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
